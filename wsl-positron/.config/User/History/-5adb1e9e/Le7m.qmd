---
title: "Technical Reference for Applying Machine Learning in Predicting Medication Treatment Outcomesfor Opioid Use Disorder"
author: "Raymond R. Balise, PhD with Kyle Grealis, MS and Gabriel Odom, PhD"
date: last-modified
format:
  pdf
knitr:
  opts_chunk:        ########## set global options ############
    collapse: true   # keep code from blocks together (if shown)
    echo: false      # don't show code
    message: true    # show messages
    warning: true    # show warnings
    error: true      # show error messages
    comment: ""      # don't show ## with printed output
    dpi: 100         # image resolution (typically 300 for publication)
    fig-width: 6.5   # figure width
    fig-height: 4.0  # figure height
    R.options:    
      digits: 3      # round to three digits
editor: source
---

```{r tidyverse-tidymodels}
#| echo: false

library(conflicted)
suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()
suppressPackageStartupMessages(library(tidyverse))

# suppress "`summarise()` has grouped output by " messages
options(dplyr.summarise.inform = FALSE)
```

```{r other-packages}
#| echo: false

library(kableExtra)
```

This is a highly abridged technical reference for the modeling done in "Applying Machine Learning in Predicting Medication Treatment Outcomes for Opioid Use Disorder" which is under review. For more details please see:  [https://CTN-0094.github.io/ml_paper_2025](https://CTN-0094.github.io/ml_paper_2025). 

## Inclusion Criteria

Of all the individuals who were randomized in the three trials <mark>(N = 2,492), 99.4% (all except 14 people)</mark> had _some_ urine drug screening information.  Thus, the analysis cohort consisted of the <mark>2,478</mark> people where there was any information on their drug use during treatment. 

## Variables/Features

```{r}
#| label: tbl-one
#| tbl-cap: Features used to predict treatment failure
#| echo: false
tribble(
  ~Feature, ~Details,
  # Demographics
  'Age', 'Numeric',
  'Ethnicity (is Hispanic)', 'Yes, No, Unknown',
  'Race', 'Black, White, Other',
  'Unemployed', 'Yes, No, Unknown',
  'Stable Housing', 'Yes, No, Unknown',
  'Education', 'Missing, Less than HS, HS or GED, More than HS',
  'Marital Status', 'Unknown, Never married, Married or Partnered, Separated/Divorced/Widowed',
  'Sex (is Male)', 'Yes, No, Unknown',
  # Drug Use
  'Smoking History', 'Yes, No, Unknown',
  'Fagerstrom Test for Nicotine Dependence', 'Numeric',
  'IV Drug use History', 'Yes, No, Unknown',
  # History & Physical
  'Pain Closest to Enrollment', 'None, Very mild to moderate, Severe',
  'Schizophrenia', 'Yes, No, Unknown',
  'Depression', 'Yes, No, Unknown',
  'Anxiety', 'Yes, No, Unknown',
  'Bipolar', 'Yes, No, Unknown',
  'Neurological Damage', 'Yes, No, Unknown',
  'Epilepsy', 'Yes, No, Unknown',
  # Comorbid Drug Use Diagnoses
  'Alcohol', 'Yes, No, Unknown',
  'Amphetamines', 'Yes, No, Unknown',
  'Cannabis', 'Yes, No, Unknown',
  'Cocaine', 'Yes, No, Unknown',
  # Treatment Medication
  'Study Site', 'Clinic Number',
  'Clinic Type', 'Inpatient, Outpatient',
  'Medication', 'Inpatient BUP, Inpatient NR-NTX, Methadone, Outpatient BUP, Outpatient BUP + Enhanced Medical Management, Outpatient BUP + Standard Medical Management',
  # Drugs Used in Past 28 Days
  'Number of Distinct Substances', 'Numeric',
  'Number of Days with Any Use', 'Numeric'
) |> 
  kable() |> 
  column_spec(1, width = "7cm") |> 
  column_spec(2, width = "10cm") |> 
  kable_styling(latex_options = "striped")
```

## Training Testing Split and Validation

The analysis data was initially split using a stratification algorithm that assured the same percentage of people experienced treatment success in the training dataset (3/4 of the data) and the testing dataset (1/4 the data).  For model tuning, five-fold cross validation was used.

## Final Recipe

The preprocessing recipe followed the steps listed below.  Algorithm details are covered in the documentation for the R `recipes` package (Version 1.3.1).

1. For all predictors, remove any variables with zero or near zero variance.  See `step_nzv()` documentation.
2. For all nominal predictors, any string variables are converted to categorical factors. See the `step_string2factor()` documentation.
3. For all predictors, all missing values are imputed using a k = 5 nearest neighbors algorithm. See the `step_impute_knn()` documentation.
4. For all nominal predictors, dummy code the variables. See the `step_dummy()` documentation.
5. For all nominal predictors, pool infrequently occurring values (less than 5% of the data) into another category.  See the `step_other()` documentation.
6. For all numeric predictors, recursively remove variables that have absolute correlations > 0.9 (beginning with the highest correlation). See the `step_corr()` documentation.
7. For all numeric predictors, normalize values to have a mean of zero and a standard deviation of one. See the  `step_normalize()` documentation.

The original 46 variables are converted to features. The details for this conversion, if the recipe is applied to the full training data, are shown in the table below. Please note that the results may differ subtly across the five-fold resamples because each fold's recipe is fitted independently on that fold's analysis set. This means preprocessing parameters (like normalization estimates) and feature selection decisions (from steps like `step_corr()`, `step_nzv()`, or `step_other()`) may vary across folds, as they depend on the specific data characteristics within each fold.

```{r}
#| label: tbl-two
load('../ctn0094modeling/data/summary_of_steps.RData')

library(kableExtra)
summary_of_steps |> 
  kable() |> 
  column_spec(1, width = "5cm") |> 
  column_spec(2, width = "3cm") |> 
  column_spec(3, width = "6cm") |> 
  kable_styling(latex_options = "striped")
```

## Models

For models that tune many hyperparameters, values were selected using a space-filling parameter grid instantiated using the `dials::grid_latin_hypercube()` function.

### Logistic Regression

A standard logistic model was fit using the default `glm.fit` method in `stats::glm()`.

### Logistic Regression Via Lasso

A logistic model, allowing for the same resampling estimates for all other models, was fit using `glmnet` engine configured to run a lasso model (`mixture = 1`) but with a minuscule $10^{-10}$ penalty.

### LASSO

A LASSO model was fit using the `glmnet` engine (`mixture = 1`). Preliminary tuning was run across 30 samples between $10^{-10}$ to $1$. After examining the ROC estimates, the model was revised to use 30 equally spaced values across a penalty range of  $10^{-3}$ to $1$.
  

### KNN

A KNN model was fit with using the `kknn` engine.  The model was trained with a space-filling parameter grid with 50 combinations across 1 to 50 neighbors, nine weight functions (i.e., 'rectangular', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'cos', 'inv', 'gaussian', and 'rank') and Minkowski Distance Order  (range: [1, 2]).

### MARS

A MARS model was fit with `earth` engine tuned across five levels of the degree of interaction from one to five using a backwards pruning method.

### CART

A CART model was fit with the `rpart` engine.  The model was trained with a space-filling parameter grid with 50 combinations across tree depth (range: [1, 15]), minimal node size (range: [2, 40]), and cost complexity (range: [$10^{-10}$, $10^{-1})$])

### Random Forest

A Random Forest model was fit with the `randomForest` engine.  The model was trained with a space-filling parameter grid with 50 combinations across minimal node size (range: [2, 40]) and number of randomly selected predictors (range: 1 to an estimated finalized during training).

### XGBoost

A boosted tree model was fit using the XGBoost algorithm with the `xgboost` engine.  The model was trained with a space-filling parameter grid with 50 combinations across tree depth (range: [1 to 15]), minimal node size (range: [2 to 40]), minimal loss reduction (range: [$10^{-10}$, $10^{1.5}$]), sample size (range: [0.1, 1]), randomly selected predictors (range: 1 to an estimated finalized during training), and learn rate (range: [$10^{-10}$, $10^{-1}$])

### BART

A Bayesian additive regression tree model was fit using the `dbarts` engine.   The model was trained with a space-filling parameter grid with 50 combinations across trees (range: [1, 2000]), terminal node prior coefficient (range: (0, 1])), terminal node prior exponent (range: (1, 3]), and the prior for outcome range (range: (0, 5]).

### Support Vector Machine

A support vector machine model was fit using the `kernlab` engine.  The model was trained using parameters from a regular grid with 10 values across cost (range: [$10^{-10}$, $10^{5}$]) and polynomial degree (range: [1, 3]).

### Neural Network

A single layer neural network was fit using the `brulee` engine.  The model was trained with a space-filling parameter grid with 30 combinations across hidden units (range: [10, 100]),  amount of regularization (range: [$10^{-5}$, $1$]), and learning rate (range: [$10^{-10}$, $10^{-1}$]) with 100 epochs.


