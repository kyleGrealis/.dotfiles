---
title: "Supplement to *Applying Machine Learning in Predicting Medication Treatment Outcomes for Opioid Use Disorder*"
author: "Raymond R. Balise, Kyle Grealis, Guertson Jean-Baptiste, and the CTN-0094 Team"
date: today
format: 
  html: 
    page-layout: full
    css: 
      - callout.css
      - styles.css
    code-line-numbers: false
    code-link: true
    toc: true
    embed-resources: true
editor: source
knitr:
  opts_chunk:        ########## set global options ############
    collapse: true   # keep code from blocks together (if shown)
    message: true    # show messages
    warning: true    # show warnings
    error: true      # show error messages
    comment: ""      # don't show ## with printed output
    dpi: 100         # image resolution (typically 300 for publication)
    fig-width: 6.5   # figure width
    fig-height: 4.0  # figure height
    R.options:    
      digits: 3      # round to three digits
csl: the-new-england-journal-of-medicine.csl
bibliography: 
  - references.bib
  - packages.bib
---

```{r tidyverse-tidymodels-and-friends}
#| echo: false

library(conflicted)
conflict_prefer("filter", "dplyr", quiet = TRUE)
conflict_prefer("lag", "dplyr", quiet = TRUE)

suppressPackageStartupMessages(library(tidyverse))

suppressPackageStartupMessages(library(tidymodels))
# suppress "`summarise()` has grouped output by " messages
options(dplyr.summarise.inform = FALSE)

suppressPackageStartupMessages(library(flextable))
suppressPackageStartupMessages(library(ggthemes))
suppressPackageStartupMessages(library(gt))
suppressPackageStartupMessages(library(labelled))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(scales))
suppressPackageStartupMessages(library(table1))
```

```{r load-data}
#| echo: false
# analysis <- readRDS("../ctn0094modeling/data/analysis.rds")
# skim_analysis <- readRDS("../ctn0094modeling/data/skim_analysis.rds")

# load("../ctn0094modeling/data/a_test.RData")
# load("../ctn0094modeling/data/a_train.RData")

# load("../ctn0094modeling/data/knn_metrics.Rdata")
# load("../ctn0094modeling/data/knn_autoplot.RData")
# load("../ctn0094modeling/data/knn_conf_mat.Rdata")

analysis <- readRDS("./ext/data/analysis.rds")
skim_analysis <- readRDS("./ext/data/skim_analysis.rds")

load("./ext/data/a_test.RData")
load("./ext/data/a_train.RData")

load("./ext/data/knn_metrics.Rdata")
load("./ext/data/knn_autoplot.RData")
load("./ext/data/knn_conf_mat.Rdata")
```

## What is this?

This is a companion to the paper "Understanding the Utility of Machine Learning for Predicting Medication Assisted Treatment Outcomes for Opioid Use Disorder" which is under review for publication. That paper contains the summary of modeling results when attempting to predict failure of treatment for people undergoing treatment for Opioid Use Disorder (OUD). Here you can learn about the machine learning (ML) methods we use and how to do them. We begin by describing the workflow and methods, then we explore the differences between ML and traditional statistics and, finally, we explain the code. Our goal is to provide a friendly introduction to the ML topics and to make it easy to start to learn the coding.

## Introduction

Before we discuss the models themselves, let's consider the project workflow which starts with the participant data and ends with a set of interpretable ML models.

You can download the complete analysis workflow for the project from GitHub by using this <mark>[hyperlink](https://github.com/CTN-0094/ml_paper_2025){target="_blank"}</mark>, which will be live after the paper is accepted. You can find the necessary software to install on the project's GitHub page. If you have not worked with GitHub before, you can study the code on this page as you are reading and learn about GitHub later. Everyone's favorite GitHub resource for the R world is [Happy Git and GitHub for the useR](https://happygitwithr.com/){target="_blank"}.

In the files saved to GitHub, there is an R file called ***do_everything.R*** that performs the entire modeling workflow. A copy of it appears below.


::: {.callout-note title="do_everything.R" collapse="true" icon="false" appearance="minimal"}

**DON’T PANIC** if you don’t speak R—it’s included as a reference for those familiar with the language. If you’re a novice, just note the following:

- Lines that begin with <code>#'</code> are **technical explanations**.  
- Lines that begin with <code>#</code> are **human-readable comments**.  

We’ll discuss the details below, but the key point is this:

> There is **one code file** that can fully reproduce the results in the paper.

Because the project is complex—and some parts take *hours* to run—the **_do_everything.R_** script tells R to execute other code files (with names ending in `.R`) that contain the modeling instructions for different machine learning algorithms.

By splitting the code into many smaller `.R` files, we were able to test each chunk independently without having to rerun the entire workflow.

```{.r}
{{< include ext/modeling/do_everything.R >}}
```
:::

The ***do_everything.R*** code takes several hours to run on a modern workstation (e.g., a 2023 M2 Ultra Mac Studio with 192 GB of RAM), so you probably want to run the workflow overnight.

The source of the participants and predictor variables are described in general terms in the paper. Below, before we introduce the modeling concepts and procedures, we provide more details to allow you to familiarize yourself with the participants and explore the data.

The individual variables are documented in the `public.ctn0094data` R package described [here](https://ctn-0094.github.io/public.ctn0094data/){target="_blank"}. If you run the steps to load the R software libraries (***libraries.R***) and load the data (***load.R***) described in the workflow below, you will have access to the analysis dataset.

## The Workflow

When you view the ***do_everything.R*** file, you will notice its design to be a wrapper to create project-specific functions and execute several other R programs.

The preprocessing of the data begins by loading the required R packages using the ***libraries.R*** file:

::: {.callout-note title="libraries.R" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/libraries.R >}}
```
:::

<!-- There is a file that loads the failure of treatment information. We have saved that file to speed up the run time of the code but you can see the algorithm and details in the ***create_outcome.R*** file:

::: {.callout-note title="create_outcome.R" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/create_outcome.R >}}
```
::: -->

The modeling dataset, which is called `analysis`, is created using a lot of `tidyverse` code, relying heavily on the `dplyr` package. If you are not familiar with the `tidyverse` dialect of R, follow the advice found here: [https://www.tidyverse.org/learn/](https://www.tidyverse.org/learn/){target="_blank"}. Many details of the data preparation are discussed below. The code itself is in the ***load.R*** file:

::: {.callout-note title="load.R" collapse="true" icon="false" appearance="minimal"}

> This creates the `analysis` dataset. We left some of the code for alternative models we tried and for conducting a sensitivity analysis.

```{.r}
{{< include ext/modeling/load.R >}}
```
:::

The modeling and model-specific data preprocessing is performed using the `tidymodels` ecosystem of R packages. The best places to learn about `tidymodels` are [https://www.tidymodels.org/](https://www.tidymodels.org/){target="_blank"} and [https://www.tmwr.org/](https://www.tmwr.org/){target="_blank"}. The concepts, pieces, and relevant details we use are explained below. The model-specific preprocessing is located in the ***preprocess_recipe.R*** file:

::: {.callout-note title="preprocess_recipe.R" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/preprocess_recipe.R >}}
```
:::

We've organized the code for each model workflow we use in its own file. For example, the CART modeling is in the ***CART.R*** file. We will later explain the code after first introducing key ML concepts.

After completing the modeling, we summarize the results using an R script called ***summarize.R***:

::: {.callout-note title="summarize.R" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/summarize.R >}}
```
:::

Finally, we apply interpretable ML tools from the `DALEX` and `DALEXtra` R packages to reprocess the data using our modeling results. This process involves repeatedly applying the model to estimate performance, as if on new participants. Due to its iterative nature, these steps take several hours to complete. The code can be found in ***vip_calcs.R***:

::: {.callout-note title="vip_calcs.R" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/vip_calcs.R >}}
```
:::

## Participant Details

### Subjects

Of the `r public.ctn0094data::randomization |> filter(which == 1) |> nrow() |> comma()` people who were randomized, a total of `r nrow(analysis) |> scales::comma()` individuals were used for the analysis. A total of `r public.ctn0094data::randomization |> filter(which == 1) |> nrow() - nrow(analysis)` people were excluded because they had no self-reported drug use information before and after randomization. The `analysis` dataset is created by the code in `load.R`. Most of the steps to create `analysis` are simple transforms and joins to merge many database tables but because some of the subjects in CTN-0030 were randomized twice, the `randomization` table requires a filtering step. For all analyses, only the first treatment was used.

::: {.callout-note title="Code to pre-process the `subjects` dataset" collapse="true" icon="false" appearance="minimal"}

```{r code-to-make-subjects}
#| eval: false

subjects <-
  public.ctn0094data::randomization |>
  filter(which == 1) |>
  inner_join(everybody, by = "who") |>
  rename(rand_date = when) |>
  select(project, who, treatment, rand_date) |>
  mutate(
    medication = case_when(
      treatment == "Inpatient BUP" ~ "Buprenorphine",
      treatment == "Inpatient NR-NTX" ~ "Naltrexone",
      treatment == "Methadone" ~ "Methadone",
      treatment == "Outpatient BUP" ~ "Buprenorphine",
      treatment == "Outpatient BUP + EMM" ~ "Buprenorphine",
      treatment == "Outpatient BUP + SMM" ~ "Buprenorphine"
    ),
    medication = factor(medication),
    in_out = case_when(
      treatment == "Inpatient BUP" ~ "Inpatient",
      treatment == "Inpatient NR-NTX" ~ "Inpatient",
      treatment == "Methadone" ~ "Outpatient",
      treatment == "Outpatient BUP" ~ "Outpatient",
      treatment == "Outpatient BUP + EMM" ~ "Outpatient",
      treatment == "Outpatient BUP + SMM" ~ "Outpatient"
    ),
    in_out = factor(in_out)
  )
```
:::

### Features/Variables

::: callout-note
The terms "feature" and "variable" each refer to the details of a treatment outcome predictor. Typically the two terms can be used interchangeably. A yes/no variable, like "used cocaine", is a single feature in the model. Things get complicated when a predictor can be converted to multiple features. For example, education can be converted to a series of yes/no indicators like "graduated grade school", "attended high school but did not graduate", and "graduated high school". In this case, "education" is a feature and so is "high school graduate". Similar ambiguities arise when an algorithms splits a continuous variable like age into subgroups. Specifically, age with a numeric value and categorical "age \> 60" can be features. The subtle difference, between using a variable as a predictor versus using groups or levels from the variable as predictors can hopefully be inferred from context.
:::

The output below contains descriptive statistics on the `analysis` dataset which includes categorical variables, known in R as factors, and numeric variables. In R, factors are categorical variables that have a fixed and known set of possible values. Below, these data are described using the R package `skimr`. The `skim()` function generates summary statistics, presenting them in separate tables for factor and numeric variables. In both tables, each variable is listed in a column titled `skim_variable`, with its corresponding summary statistics presented in the adjacent columns:

-   `n_missing` displays the total number of missing values.
-   `complete_rate` shows a particular variable's percentage of non-missing values; a `complete_rate` of 1.00 corresponds to 100% completed records and would therefore have a `n_missing` of 0.
-   `ordered` is used to indicate whether the factor variable should be ordered in some way (e.g., small, medium and large are ordered categories).
-   `unique` identifies the total number of distinct categorical factors.
-   `top_counts` shows an abbreviated version of the categories and a count of the records.

::: {.callout-note title="Show Predictor Statistics - Factors" collapse="true" icon="false" appearance="minimal"}

```{r create-report}
#| eval: false
#| echo: false

# This section is not evaluated! ----

analysis <- readRDS("~/Documents/GitHub/CTN-0094/ctn0094modeling/data/analysis.rds")
skim_analysis <- skimr::skim(analysis)
write_rds(skim_analysis, file = "data/skim-analysis.rds")
write_rds(skim_analysis, file = "./ext/data/skim_analysis.rds")
# copy over file
# skim_analysis <- readRDS("~/Documents/GitHub/CTN-0094/ml_paper_2022/skim_analysis.rds")
skim_analysis <- readRDS("./ext/data/skim_analysis.rds")
```

```{r print-skim-factors}
#| echo: false

factors <- skim_analysis |> 
  filter(skim_type == "factor") |> 
  select(-skim_type, -starts_with("numeric"))

knitr::kable(factors)
```

:::

The table for numeric data is shown in the dropdown below (do notice that the table scrolls left-to-right) and provides summary statistics such as the mean, the standard deviation (`sd`), a histogram, and the quartiles (Q~0~, Q~1~, Q~2~, Q~3~, Q~4~) labeled `p0`, `p25`, `p50`, `p75`, and `p100` respectively. Each quartile value represents the number that separates each quarter. Therefore, Q~0~ is the smallest value in the data and Q~4~ would be the largest. Values between Q~0~ and Q~1~ detail the 25% lowest values in the data, values between Q~1~ and Q~2~ are the next 25%, etc. Lastly, a histogram is included to visualize the data distribution.

::: {.callout-note title="Show Predictor Statistics - Numeric" collapse="true" icon="false" appearance="minimal"}

> This table is scrollable left-to-right.

```{r print-skim-numerics}
#| echo: false

numerics <- skim_analysis |> 
  filter(skim_type == "numeric") |> 
  select(-skim_type, -starts_with("factor"))

knitr::kable(numerics)
```

:::

Full documentation for the dataset tables and descriptions of their variables can be found [here](https://ctn-0094.github.io/public.ctn0094data/reference/index.html){target="_blank"}.

```{r psych-agreement}
#| echo: false

phi_schiz <-
  public.ctn0094data::psychiatric |>
  filter(
    schizophrenia %in% c("Yes", "No"),
    has_schizophrenia %in% c("Yes", "No")
  ) |>

  select(schizophrenia, has_schizophrenia) |>
  group_by(schizophrenia, has_schizophrenia) |>
  count() |>
  pull(n) |>
  psych::phi()

phi_dep <-
  public.ctn0094data::psychiatric |>
  filter(
    has_major_dep %in% c("Yes", "No"),
    depression %in% c("Yes", "No")
  ) |>
  select(has_major_dep, depression) |>
  group_by(has_major_dep, depression) |>
  count() |>
  pull(n) |>
  psych::phi()

phi_anx <-
  public.ctn0094data::psychiatric |>
  filter(
    anxiety %in% c("Yes", "No"),
    has_anx_pan %in% c("Yes", "No")
  ) |>
  select(anxiety, has_anx_pan) |>
  group_by(anxiety, has_anx_pan) |>
  count() |>
  pull(n) |>
  psych::phi()
```

The model feature details are shown in [Table 1](#table1). For approximately 1,500 participants, schizophrenia, depression and anxiety were assessed using both a medical/psychiatric interview and the ASI-Lite questionnaire.[@Cacciola2007] The agreement between the two sources shows little/weak agreement ($\phi$ is `r phi_schiz`, `r phi_dep`, `r phi_anx` respectively). Therefore, we created composite indicators that scored a participant as affected if they were positive on either measure and negative otherwise. *A priori*, it was unclear how to handle treatment regimen and recent drug use history in the modeling process. Treatments were initially grouped using study-specific treatment arms (with six levels). However, such groupings are not helpful for participants who wish to generalize our results. Therefore, we created two indicators variables: one representing the study drug and the other representing inpatient vs. outpatient care.

Recent drug use was also processed in two ways. One approach, reflected in the variables presented in [Table 1](#table1) and [Table 2](#table2), uses many variables, such as those indicating comorbid substance use diagnoses, the total number of distinct drugs used in the past 28 days, the presence and absence of specific drugs, the amount of use for specific drugs, and the number of days where at least one drug was used. The other method was to assign each participant to a polysubstance drug use profile based on a previously published latent class analysis.[@pan2022] The latter method is somewhat problematic in that the drug use profiles were previously built using all the data. While groupings were constructed irrespective of the outcome, this process did not involve using resamples when building latent classes in the subsets. This violates of the premise that feature engineering and preprocessing should be encapsulated inside of resamples (like the 10-fold cross-validation described below).[@kuhn2019] The details of resampling will be explained in the next few sections. Currently, the statistical theory and software tools necessary to properly handle the tuning and "averaging" of results from latent class analysis across repeated samples are underdeveloped. This includes adequately accounting for the uncertainty in the estimates. Consequently, we opted for a strategy utilizing many variables rather than reducing variables through latent class analysis.

{{< pagebreak >}}

````{=html}
<p id="table1"></p>
<!-- ::: {.scrollable-table}
```{r table-variables-used}
#| message: false
#| echo: false

variables_table <-
  read_csv(
    # "variablesTable.csv",
    "./ext/data/variablesTable.csv",
    col_types = cols(
      ...4 = col_skip(), ...5 = col_skip(), ...6 = col_skip()
    )
  ) |>
  filter(Details != "BUP, NR-NTX, Methadone") |>
  as_grouped_data(groups = "Group")

as_flextable(variables_table, hide_grouplabel = TRUE) |>
  bold(j = 1, i = ~ !is.na(Group), bold = TRUE, part = "body") |>
  bold(part = "header", bold = TRUE) |>
  prepend_chunks(i = ~ is.na(Group), j = 1, as_chunk("\t")) |>
  width(j = "Feature", 4, unit = "in") |>
  width(j = "Details", 4, unit = "in") #|>
  # add_footer_lines("Table 1:  Features used to predict failure of treatment.") |>
  # italic(italic = TRUE, part = "footer")
```
::: -->
````

::: my-gt-table1
```{r}
#| message: false
#| echo: false

table_one <- 
  tribble(
  ~Feature, ~Details,
  # Demographics
  'Age', 'Numeric',
  'Ethnicity (is Hispanic)', 'Yes, No, Unknown',
  'Race', 'Black, White, Other',
  'Unemployed', 'Yes, No, Unknown',
  'Stable Housing', 'Yes, No, Unknown',
  'Education', 'Missing, Less than HS, HS or GED, More than HS',
  'Marital Status', 'Unknown, Never married, Married or Partnered, Separated/Divorced/Widowed',
  'Sex (is Male)', 'Yes, No, Unknown',
  # Drug Use
  'Smoking History', 'Yes, No, Unknown',
  'Fagerstrom Test for Nicotine Dependence', 'Numeric',
  'IV Drug use History', 'Yes, No, Unknown',
  # History & Physical
  'Pain Closest to Enrollment', 'None, Very mild to moderate, Severe',
  'Schizophrenia', 'Yes, No, Unknown',
  'Depression', 'Yes, No, Unknown',
  'Anxiety', 'Yes, No, Unknown',
  'Bipolar', 'Yes, No, Unknown',
  'Neurological Damage', 'Yes, No, Unknown',
  'Epilepsy', 'Yes, No, Unknown',
  # Comorbid Drug Use Diagnoses
  'Alcohol', 'Yes, No, Unknown',
  'Amphetamines', 'Yes, No, Unknown',
  'Cannabis', 'Yes, No, Unknown',
  'Cocaine', 'Yes, No, Unknown',
  # Treatment Medication
  'Study Site', 'Clinic Number',
  'Clinic Type', 'Inpatient, Outpatient',
  'Medication', 'Inpatient BUP, Inpatient NR-NTX, Methadone, Outpatient BUP, Outpatient BUP + Enhanced Medical Management, Outpatient BUP + Standard Medical Management',
  # Drugs Used in Past 28 Days
  'Number of Distinct Substances', 'Numeric',
  'Number of Days with Any Use', 'Numeric'
)

table_one_gt_tbl <-
  table_one |> 
  gt() |> 
  cols_width(Feature ~ px(350)) |> 
  cols_label(
    Feature = md('**Feature**'),
    Details = md('**Details**')
  ) |> 
  # tab_footnote(
  #   footnote = 'Table 1: Features used to predict failure of treatment.'
  # ) |> 
  tab_row_group(
    label = 'Drugs Used in Past 28 Days',
    rows = 26:27
  ) |> 
  tab_row_group(
    label = 'Treatment Details',
    rows = 23:25
  ) |> 
  tab_row_group(
    label = 'Comorbid Drug Use Diagnoses',
    rows = 19:22
  ) |> 
  tab_row_group(
    label = 'History & Physical',
    rows = 12:18
  ) |> 
  tab_row_group(
    label = 'Drug Use',
    rows = 9:11
  ) |> 
  tab_row_group(
    label = 'Demographics',
    rows = 1:8
  ) |> 
  tab_style(
    style = list(
      # cell_fill(color = '#d3d3d3'),
      cell_text(weight = '600'),
      cell_text(style = 'italic')
    ),
    locations = cells_row_groups(groups = everything())
  ) |> 
  tab_style(
    style = css(position = "sticky", top = px(-2), zIndex = 100),
    locations = cells_column_labels()
  ) |> 
  tab_options(
    table.width = pct(100),
    container.height = px(450),
    container.overflow.x = TRUE,
    container.overflow.y = TRUE,
    container.padding.y = px(0),
    row_group.padding = px(10),
    data_row.padding = px(3),
    data_row.padding.horizontal = px(30),
    column_labels.font.size = px(18),
    column_labels.background.color = '#d3d3d3',
    column_labels.border.top.color = 'black',
    # column_labels.border.bottom.color = 'black',
    # column_labels.border.bottom.width = px(2.5),
    row_group.border.top.color = 'black',
    row_group.border.bottom.color = 'black',
    row_group.border.bottom.width = px(2),
    row_group.background.color = '#eeeded',
    quarto.disable_processing = TRUE  # set TRUE to remove Quarto autoformat & striping
  ) |> 
  opt_row_striping(row_striping = FALSE)

table_one_gt_tbl
```

:::

::: table-footer
**Table 1: Features used to predict failure of treatment.**
(This table scrolls vertically)
:::

{{< pagebreak >}}

```{=html}
<p id="table2"></p>
```

:::: sticky-table-container
::: sticky-table
```{r table-variables-used-descriptives}
#| eval: true
#| message: false
#| echo: false

# final_analysis <- read_rds("../ctn0094modeling/data/analysis.rds")
# final_analysis <- read_rds("./ext/data/analysis.rds")
final_analysis <- analysis

label(final_analysis$trial) <- "Trial"
label(final_analysis$medication) <- "Medication"
label(final_analysis$in_out) <- "Type"
label(final_analysis$used_iv) <- "Used IV Drugs"
label(final_analysis$age) <- "Age"
label(final_analysis$race) <- "Race"
label(final_analysis$is_hispanic) <- "Hispanic"
label(final_analysis$job) <- "Employment"
label(final_analysis$is_living_stable) <- "Stable Housing"
label(final_analysis$education) <- "Education"
label(final_analysis$marital) <- "Marital Status"
label(final_analysis$is_male) <- "Is Male"
label(final_analysis$is_smoker) <- "Is Smoker"
label(final_analysis$per_day) <- "Cigarettes Per Day"
label(final_analysis$ftnd) <- "Fagerstrom Test for Nicotine Dependence"
label(final_analysis$pain) <- "Pain at Enrollment"
label(final_analysis$any_schiz) <- "Schizophrenia"
label(final_analysis$any_dep) <- "Depression"
label(final_analysis$any_anx) <- "Anxiety"
label(final_analysis$has_bipolar) <- "Bipolar"
label(final_analysis$has_brain_damage) <- "Brain Damage"
label(final_analysis$has_epilepsy) <- "Epilepsy"
label(final_analysis$has_alcol_dx) <- "Alcohol Diagnosis"
label(final_analysis$has_amphetamines_dx) <- "Amphetamines Diagnosis"
label(final_analysis$has_cannabis_dx) <- "Cannabis Diagnosis"
label(final_analysis$has_cocaine_dx) <- "Cocaine Diagnosis"
label(final_analysis$has_sedatives_dx) <- "Sedatives Diagnosis"
label(final_analysis$is_homeless) <- "Is Homeless"
label(final_analysis$did_use_cocaine) <- "Used Cocaine"
label(final_analysis$did_use_heroin) <- "Used Heroin"
label(final_analysis$did_use_speedball) <- "Used Speedball"
label(final_analysis$did_use_opioid) <- "Used Opioid"
label(final_analysis$did_use_speed) <- "Used Stimulants"
label(final_analysis$days_cocaine) <- "Days Using Cocaine"
label(final_analysis$days_heroin) <- "Days Using Heroin"
label(final_analysis$days_speedball) <- "Days Using Speedball"
label(final_analysis$days_opioid) <- "Days Using Opioids"
label(final_analysis$days_speed) <- "Days Using Stimulants"
label(final_analysis$days_iv_use) <- "Days Using IV Drugs"
label(final_analysis$shared) <- "Shared Needles"
label(final_analysis$tlfb_days_of_use_n) <- "TLFB Days of Drug Use"
label(final_analysis$tlfb_what_used_n) <- "TLFB Number Drugs Used"
label(final_analysis$withdrawal) <- "Withdrawal Severity"
label(final_analysis$detox_days) <- "Days in Detox (NA approx 0)"
label(final_analysis$did_relapse) <- "Did Relapse"

final_analysis |>
  table1::table1(
    ~. | trial, 
    data = _, 
    footnote = "Table 2:  Descriptive statistics. (This table scrolls vertically)"
  )
```

:::
::::

### The Endpoint to Predict

As mentioned in the paper, though many methods have been proposed to evaluate the treatment success for OUD, we opted to use the Lee, et al. definition which assesses weeks to relapse. It uses urine drug screening starting at day 21 post-randomization and regards four consecutive weeks with positive and/or missing urine drug screening as "positive".[@lee2018] The code to calculate the definition of relapse used in the paper, along with dozens of other definitions, is available through [CTNote](https://ctn-0094.github.io/CTNote/){target="_blank"}, an open-source software library for the R language.[@odom; @brandt] A summary of the code used here can be found in the ***load.R*** file shown above.

## Machine Learning Concepts

### The Big Picture

The predictive modeling typically done with ML methods has different goals compared to the traditional (Neyman-Pearson) *p*-value-based hypothesis testing familiar to most clinical investigators. A *p*-value-based approach is useful in the context of experiments where the goal is testing for differences between groups. Traditional hypothesis testing begins with the idea that *an experiment* was conducted at great expense and the goal is to make a statement about how unlikely the observed effect was to appear by chance alone. However, the broad application of *p*-value-based statistics, especially if the goal is to produce replicable predictive modeling, has been fraught with issues.[@wasserstein2016] Of particular concern is that every effect (even a microscopic difference in the mean response to two treatments) becomes statistically significant with a large enough sample. This notion, mixed with a common misconception that *p*-values measure the size of a treatment effect, when in fact they do not, suggests that investigators should be interpreting different metrics to evaluate model performance instead of relying solely on *p*-values.

The desire to shift the focus away from *p*-value-based model assessments has led to the demand by statisticians, data scientists and other ML aficionados to report on the precision of estimates, typically in the form of confidence (or credible) intervals. Interestingly, the traditionally taught confidence interval formulas, which rely on theoretical assumptions about the data, are being largely supplanted by ML methods, like bootstrap, championed by experts who move fluidly between theoretical statistics and machine learning algorithms.[@efron1994]

In contrast to the traditional experimental approach of overemphasizing *p*-values, ML begins with the premise that some data was collected, ideally a lot of data, and the goal is usually *to make predictions about an outcome for a new observation*. There is a lot of common ground between traditional statistics and ML. The machinery that statisticians created long before the rise of ML can be (and is) used to calculate estimates with valid confidence intervals and also to make predictions. In fact, many ML projects begin by using traditional statistical techniques as the first step, such as linear or logistic regression. The next steps differ where ML experts typically deprecate the *p*-values produced by the initial models and quickly move on to trying many other algorithms.

There is a plethora of statistical and ML methods for a multitude of outcomes. While ML methods can be used to predict continuous outcomes (morphine milligram equivalents consumed in a day), count data (the number of used needles brought to a harm reduction center), or time until an event (days until overdose after discharge), we will focus on predicting a binary outcome: treatment success or failure in Medications for Opioid Use Disorder (MOUD) programs.

### The Role of Training and Testing Datasets

The ML process begins by setting aside a small part of the data as a technique to focus on predicting future samples. This data split is typically between 10% to 25% of the collected data and are eventually used to evaluate or *test* the performance of a predictive model that was *trained* on the remaining split of the data. In the context of predicting treatment success or failure, there is a yes/no outcome variable for each person in the test set. This outcome variable holds the truth about whether a person is still using drugs after an intervention.

An algorithm is selected, such as one of those described below, and applied to the *training* dataset. This algorithm produces an optimal set of rules (or a formula) for predicting success or failure for the people in the *training* dataset. The quality of that training model can be assessed using the small sample that was set aside and labeled the *test* or *testing* data. Predictions for people in the *testing* data can be generated by applying the predictive model built from the training data. Then the predictions are checked against the *true* values in the testing data.

It is important to fully consider the implications of the training and testing split. Specifically, realize that first applying the training data to construct the model and then assess against the testing data mirrors the process that would occur if the treatments were repeated on a new sample from the same population. This is the gold standard for evaluating this population. However, like all clinical investigations, a key question is "are these study participants like my study participants?" This answer is addressed by studying "Table 1" of a paper. Just like the optimal model describing harm reduction policies in rural Zimbabwe may not be particularly useful for policy makers in New York City, the value of the test data is its ability to describe what would happen in another data set *from a similar cohort*. Here, as outlined in the paper, we build predictive models using data from three of the largest clinical MOUD trials which were harmonized as part of a NIDA-sponsored project CTN-0094.[@raymondbalise2024]

### Choosing a Metric of Success

Someone evaluating a yes/no question, such as whether a person stopped using a drug, can generate a large number of evaluation metrics.[@brandt] Some, such as accuracy, are relatively obvious. That is, everyone wants to know what percentage of the participants in the testing dataset were correctly called/guessed as either "using" or "no longer using" a drug after an intervention designed to produce cessation. When you think about scenarios where most participants are (un)successful in treatment, a weakness with accuracy becomes apparent. If 95% of participants fail treatment, a model which ignores all of the predictors and just specifies that *everyone* will fail, will still be 95% accurate. So, instead of just focusing on accuracy, insightful researchers use additional metrics to evaluate the quality of a predictive model. To understand these metrics, study the information in the "confusion matrix", shown in [Figure 1](#fig:two_by_two). A confusion matrix is a two-by-two table displaying the count of the true outcomes versus the predicted outcomes.

{{< pagebreak >}}

```{r figure-two-by-two-table}
#| tbl-cap: |
#|   Figure 1:  The groups defined by a confusion matrix showing the predicted
#|   and true drug use categories after medication assisted treatment (MAT) for
#|   opioid use disorder (OUD).
#| echo: false

two_by_two <-
  bind_cols(
    `Truth` = c(
      "Actually Using: Positive (P)",
      "Actually Stopped: Actually Negative (N)"
    ),
    guess_pos = c("True Positive (TP)", "False Positive (FP)"),
    guess_neg = c("False Negative (FN)", "True Negative (TN)")
  )

header <- tibble(
  col_keys = colnames(two_by_two),
  line1 = c("Truth", rep("Prediction", 2)),
  line2 = c(
    "Truth", "Predicted to Use: \nPredicted Positive (PP)",
    "Predicted to Stop: Predicted Negative (PN)"
  )
)

flextable(two_by_two) |>
  set_header_df(
    mapping = header,
    key = "col_keys"
  ) |>
  theme_booktabs() |>
  merge_v(part = "header") |>
  merge_h(part = "header") |>
  align(align = "center", part = "all") |>
  width(width = 2.5) |>
  add_footer_lines("Figure 1:  Two-by-two table showing predicted and true drug use categories after medication assisted treatment (MAT) for opioid use disorder (OUD).") |>
  italic(italic = TRUE, part = "footer")
```

{{< pagebreak >}}

Having dealt with issues related to predicted and actual cure rates and rare outcomes for centuries, epidemiologists have developed a toolbox of metrics to describe patterns in confusion matrices. These include sensitivity ($\frac{TP}{P}$), specificity ($\frac{TN}{N}$), positive predictive value (PPV) ($\frac{TP}{PP}$) and negative predictive value (NPV) ($\frac{TN}{PN}$). Not being trained in epidemiology, the ML community rediscovered and relabeled the metrics. For example, sensitivity is labeled "recall" and PPV is called "precision". As annoying as the new terms may be, these ML practitioners also have popularized useful metrics like $F_1$ ($\frac{2TP}{2TP + FP + FN}$), which simultaneously summarizes both sensitivity and PPV. These ML experts have also popularized older metrics from the statistics literature like Cohen's kappa ($\frac{2 \times (TP \times TN - FN \times FP)}{(TP + FP) \times (FP + TN) + (TP + FN) \times (FN + TN)}$), which quantifies model performance beyond what is expected by chance. When there is an extreme imbalance in the number of the participants in two outcome groups like the scenario described above where accuracy is practically useless, many ML engineers create models to optimize Cohen's kappa.

Modeling practitioners agree that the overall quality of a model can typically[^1] be well described by looking simultaneously at the probability of a true positive and the probability of a false positive score.[@agresti_introduction_2019, @swets1988] This is typically expressed with a graphic called a receiver operating characteristic (ROC) curve and a numeric summary called the C-statistic (or the concordance statistic). To understand their value, consider a model which attempts to predict treatment success for participants in a clinical trial designed to help participants with an OUD diagnosis stop using opioids. After a model has been built on the training set and applied to the testing dataset, each participant in the *testing* dataset is assigned a predicted probability of treatment success. The performance of the model can be checked against the true values for these participants in the *test* set.

[^1]: excluding the extreme class imbalance scenario just described

Practically speaking, this works by varying the threshold of success. We begin by saying "*if anybody has a predicted probability of 0 or higher, label them as a success*", and check if that is the correct value for each person. Given that the minimum *possible* score is 0, by saying a probability of 0 or higher is a treatment success, the predictive model will correctly capture all the true positives but incorrectly classify all the true negatives. Next, we can increase the threshold to, say 0.1, and classify anybody who has a predicted probability of 0.1 or higher a success. That will (potentially) decrease the true positives and decrease the false positives. The same process can be repeated across the range of probabilities, up to saying only the participants who have a predicted probability of 1.0, the highest possible score, will be called a success (which will perfectly exclude the true negatives but will likely miss some of the true positives). That is, in this extreme scenario, someone who has a predicted probability of success of 0.99 would be called/guessed to have a failure of treatment because they didn't meet the requirement of a predicted probability of 1.0.

The percentage[^2] of participants correctly labelled a treatment success and the percentage *incorrectly* called a success can be plotted (see [Figure 2](#fig:roc)). The plots are typically drawn as a square and the area under the curve is shaded. This area under the curve, which typically falls between 0.5 and 1.0, captures the C-statistic (also called the receiver operating characteristic area under the curve ROC AUC). If the model performs perfectly, the plot will fill the square, and the C-statistic will be 1.0. If it only performs at chance, it will draw a 45-degree line across the plot, and the C-statistic will be 0.5. The C-statistic has a practical interpretation: if you have two participants, one who will eventually succeed in treatment and another who will fail, the C-statistic gives you the probability that the model will correctly guess which of them will be the treatment success.

[^2]: really the probability of

{{< pagebreak >}}

```{r figure-roc-1}
#| warning: false
#| fig-cap: |
#|   Figure 2: An example ROC Curve where the area under the curve, the
#|   C-statistic, is 0.94. The dotted diagonal line represents a C-statistic of
#|   0.5.
#| echo: false

# dplyr complains about the guts of roc_c
# The AUC is: roc_auc(two_class_example, truth, Class1)
roc_curve(two_class_example, truth, Class1) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw() +
  labs(
    y = "True Positive Rate (Sensitivity)",
    x = "False Positive Rate (1-Specificity)"
  ) +
  geom_ribbon(aes(ymax = sensitivity), ymin = 0, fill = "red", alpha = 0.5)
```

{{< pagebreak >}}

All that said, after building a predictive model on the training data, it is *possible* to get an assessment of how well it will perform on new participants, who are comparable to the participants in the original training data set. The "new" set of participants can be the test dataset mentioned above or it can be a different assessment set created using one of the methods described below. This focus on quantifying the practical model performance, using metrics like accuracy or ROC AUC on a sample helps address the concern about relying solely on *p*-values as mentioned above. Remember, the ML world focuses on the quality of a model's performance when it is applied to new data. Different metrics emphasize different kinds of successes or failures when the model is applied to new data. An interested reader should begin with the classic paper by Marina Sokolova and Lapalme @sokolova2009 and explore more modern papers such as those by Opitz @opitz2024 and Ganbek, Temizel and Sagiroglu @canbek2022. We chose to optimize our predictive models to produce the best ROC AUC when we apply the models to new data.

### Assessing Variable Importance

With traditional regression methods, like ordinary least squares regression, logistic regression or survival analysis, investigators typically focus on statistically significant values (p \< 0.05) and then use details like the size of beta coefficients to make statements. For example, the beta coefficients give us the number to describe the quantity of needles brought on average to a syringe exchange by men compared to women; the beta estimates for a logistic regression model can describe the odds of an overdose event for females relative to males; or the beta estimates from a survival model describe how the instantaneous risk of death goes up by some amount for someone who has started to inject heroin. While this approach still plays a role in determining the impact of predictors, most ML practitioners use other "model agnostic" methods to assess the role and importance of predictors. There are model agnostic methods to describe how changes in a predictor impact an individual (using techniques like break-down plots, Shapley additive explanation plots and ceteris-paribus profiles) as well as techniques to describe the general importance of predictors using permutation techniques. We hope to apply and describe the application of these techniques in future publications. For now, consider that the techniques which attempt to describe how an individual's outcomes change when any predictor is modified are complex, but approachable explanations can be found in publications such as Biecek and Burzykowski. @biecek2021

A useful and easy to understand concept for determining the importance of a variable is permutation-based *variable importance*. If a variable included as a predictor in a model *actually does not matter* and if its values are shuffled then assigned randomly to other participants (i.e., the data are randomly permuted), the model's performance, as measured by accuracy or the C-statistic, will not change much, if at all. If a variable *does matter* to a model, then the same random shuffling (i.e., permutation) of the data will decrease the model's performance on a measure like accuracy or the C-statistic.

This permutation method to judge the importance of variables can be used for nearly all modeling techniques. The harm done to a model's performance when a variable is permuted is graphed in a variable importance plot (VIP). VIP plots are typically drawn in one of two ways. One method is to use a bar graph where a bar for each variable stretches toward the right. In this kind of plot, longer bars represent more harm being done to the model's performance when a variable is permuted. Modern software will attempt to scale the bars using the key outcome metric (like accuracy or the C-statistic), but there is always some subjectivity in determining what is a "long" bar. Another common VIP plot uses a vertical line on the right side of the plot to indicate the model's optimized performance. It then draws bars toward the left to indicate decreases in performance if a variable is permuted. Again, a long bar indicates an important variable because shuffling and randomly assigning values to random participants hurts the model performance a lot. An alternative to just drawing a bar is to repeatedly take samples of the data and reshuffle the values again and again to see how much harm is done in the resamples. Figure 2: <i>Variable Importance Plot (VIP) for Random Forest Model</i> in the paper shows this pattern. Instead of representing a single estimate of a variable's importance with the length of a bar, a box plot can show the typical amount of harm (the shaded part of the box plot shows the middle 50%) as well as high and low estimates of the importance of each variable using the whiskers and outliers of the box plot.

### How the ML Workflow Differs from Traditional Methods

As mentioned above, a typical ML approach begins by using methods championed by statisticians for more than a generation. While the philosophy behind these methods are different, with the statistician focusing on the stochastic data model and the ML modeler focusing on the properties of the algorithms, mathematically, they are comparable.[@breiman2001; @molnar2022]

Next, before exploring the coding, we discuss the subtle differences in the data pre-processing typically done in a ML workflow. Then we discuss the role of data splitting and resampling, the cornerstones of both modern statistical methods and ML. Finally, we discuss model tuning, which is the key difference between traditional and ML methods. Along the way, we will provide real examples using *k* nearest neighbors (kNN), a conceptually simple but powerful ML algorithm.

#### Preprocessing Recipes

Traditional statistical modeling workflows involve some preprocessing of the data. For example, problematic data is identified and corrected (e.g., participants who report being 1973 years of age or subjects who died before the start of the study period). Further, outcomes are mathematically modified to ensure the appropriate theoretical assumptions are met (e.g., by taking a logarithm or applying a Box-Cox transformation on variables that cannot be well described by a bell-shaped curve). This processing can differentially impact the model metrics mentioned above (e.g., a model's accuracy or ROC AUC). The preprocessing steps that evolved for traditional modeling are used in the ML context, but the process is more complicated for two reasons: resampling/subsampling and model-specific preprocessing needs.

#### Resampling / Subsampling

First, as will be shown shortly, ML models are often built using subsamples of the training dataset. When modeling is done on subsamples, care must be taken to make sure that there is no leakage of information across the samples. For example, some modeling methods require all predictors to be on the same scale (a process which involves calculating a mean and standard deviation). When using resampling techniques, it is important that the entire dataset is not used to calculate these statistics because if all the data were to be used, the subsamples would gain information from the entire dataset (i.e., an outlier not in a subsample would still impact the overall mean). Of particular concern is the mistake of including all the data (i.e., both the training and testing data) in the preprocessing. By leaking information like the mean or standard deviation of the *entire dataset* into the training data, the faulty code can allow powerful algorithms to pick up on details that should not have been known before the testing data is touched. In other words, the models will give overly optimistic estimates when applied to the "new" testing data. These concerns are addressed by using preprocessing "recipes" which can be easily applied to subsamples.

#### Model-Specific Preprocessing

The second departure from traditional preprocessing methods is driven by the fact that different ML methods benefit from different kinds of preprocessing steps.[@silge] Some methods want categorical variables recoded as yes/no indicator variables, some methods benefit from including all the levels, and others need one level dropped. Other steps differentially help different predictive algorithms. These steps include things like dropping highly correlated predictors, combining uncommon categorical variables, and modifying datasets to account for rare events.

A host of preprocessing steps have been explored to help deal with rare outcomes. With large to huge datasets, rare outcomes can be modeled with appropriate cautions. For instance, if all the training data is used, some ML methods will do extraordinarily well at predicting the "not rare" case and poorly when predicting the "rare" outcome. This is extremely problematic when the rare category is what we most need to predict. For example, if 99% of participants have an overdose reversed with a treatment drug, ML algorithms will do a relatively good job predicting the survivors while failing to predict the fatalities. With huge data sets where the rare event happens to thousands of participants, one viable strategy is to downsample (i.e., throw away cases/records) from the more common class. That is, if there are 3 thousand people who have the rare outcome and 300,000 people who have the common outcome, 297,000 records from the common class are not used when building a sample. In contrast, it is also possible to upsample (i.e., repeat) records from the rare class. A popular variant on upsampling, which goes beyond simply duplicating records, is to create new "synthetic" cases using algorithms like SMOTE or ROSE.[@chawla2002, @menardi2012] These techniques typically reduce the quality of some metrics (i.e., decreased overall accuracy) and require more time while modeling, but these losses can be offset by large improvements in the predictions of the rare class (i.e., improved sensitivity).[@rahman2013]

With modern ML workflows it is relatively easy to apply a host of different preprocessing recipes and choose a model that successfully trades performance on one metric for another. For example, a model can be optimized to find the solution with the best accuracy, the highest ROC AUC, or one of many other metrics of success. This work is all done on the training data. Once a preprocessing recipe is paired with a modeling algorithm, that choice is applied to the testing data ***once***. It is important to remember that the testing data is held as a precious resource that is only used once. This affords us the ability to describe the model performance on a "new" sample and makes the interpretation of *p*-values less problematic compared to traditional methods where *p*-values are rarely properly adjusted for all the preliminary modeling that was done.

The paper mentions the preprocessing recipe that we used for the final analysis. We will show you that and several alternatives below in the section titled [Preprocessing Recipe].

### Model Tuning

A major difference between traditionally used modeling methods like ordinary least squares regression or logistic regression and more modern ML methods is the reliance on "tuning" the model for optimal performance. Consider the machine learning algorithm called *k*-nearest neighbors (kNN), which we will use, and discuss more, later. In a case where we are predicting treatment success for a new participant, where success is defined as a yes/no response by clinicians, kNN will find *k* "similar" participants, calculate the percentage of participants who were a treatment success or failure among them and use the majority response (i.e., "were the majority of similar participants treatment successes or failures?") to assign the outcome for the new participant. Of course, that raises two questions: how do you define "similar" and *how many similar participants* should be used? The "similarity" issue is typically resolved by plotting the data and measuring the distance between points in the plot. For example, think of a scatter plot, like the one shown in [Figure 3](fig:knn_example), which plots two potentially useful predictors of treatment success (years of drug use and drug use events in the last 30 days) on the x- and y-axes respectively. Further, imagine a training dataset with 300 participants, each of whom would be plotted as a point in the scatter plot with their outcome shown using different colored symbols for treatment success and failure. Finding people similar to our test case would be like adding a point to the scatter plot with the drug use history for our new participant and drawing concentric circles (bulls-eye rings) until we had included *k* participants. If the majority of participants in the circle were no longer using the targeted drug, we would predict treatment success for our new individual.

{{< pagebreak >}}

```{r figure-knn-example}
#| fig-cap: |
#|   Figure 3: Example data showing 123 treatment success (green filled dots)
#|   and 147 treatment failures (gray open dots) from the analysis set and one
#|   participant from the assessment set who is actually a treatment failure (red
#|   open diamond). The 3 closest neighbors from the analysis set (*k* = 3) are
#|   inside or touching the inner (black) circle and five closest neighbors
#|   (*k* = 5) are inside or touching the outer (blue) circle. With *k* = 3,
#|   100% of the neighbors are treatment failures. With *k* = 5, 80% are
#|   failures. So, this participant is correctly classified using either option of *k*.
#| echo: false

set.seed(3)
example <-
  tibble(
    `Years of Drug Use` = abs(rnorm(270, 5.0, 10)),
    `Drug use Events in Last 30 Days` = abs(rnorm(270, 5, 20)),
    success = as.factor(sample(c(0, 1), replace = TRUE, size = 270))
  )

target <- tibble(
  `Years of Drug Use` = 14.3,
  `Drug use Events in Last 30 Days` = 29.8
)


example |>
  ggplot(aes(x = `Years of Drug Use`, y = `Drug use Events in Last 30 Days`)) +
  geom_point(aes(color = success, shape = success)) +
  scale_colour_manual(
    values = c("0" = "gray", "1" = "green")
  ) +
  scale_shape_manual(values = c(21, 19)) +
  geom_point(
    aes(
      x = `Years of Drug Use`, y = `Drug use Events in Last 30 Days`
    ),
    data = target, shape = 5, color = "red", size = 2
  ) +
  geom_point(
    aes(
      x = `Years of Drug Use`, y = `Drug use Events in Last 30 Days`
    ),
    data = target, shape = 1, color = "black", size = 19.8
  ) +
  geom_point(
    aes(
      x = `Years of Drug Use`,
      y = `Drug use Events in Last 30 Days`
    ),
    data = target, shape = 1, color = "blue", size = 27
  ) +
  theme_few() +
  theme(legend.position = "none")
```

{{< pagebreak >}}

Now the more interesting question is, how many similar neighbors should we use? That is, how do we pick the best *k*? With a training dataset of 300 participants, we have a **lot** of choices for possible values of *k*: every number between 1 and 300! At the extremes, we could use the one closest person (*k* = 1), or we could use *k* = 300 people and calculate the overall probability of success in the entire training dataset. Throwing away 299 bits of information by using only the single closest person (*k* = 1) seems unwise. On the other hand, including everybody (*k* = 300), which assumes that you don't benefit from using the most similar participants, also seems like a bad idea. What we want to do is grab a "handful" of the most similar participants, test the quality of the predictions for them (perhaps with accuracy or using the ROC AUC/C-statistic), then increase the size of these "handfuls" (try *k* = 3, *k* = 5, ...).

The machine learning solution to this problem is to split the 300 participants in the training set into subgroups, which practitioners call partitions or *folds*, and then use the subgroups to evaluate across different values of *k*. If we decide to do a 10-fold analysis, we would assign each participant as being a member of one of ten subgroups/folds. We can take the first fold (which would be 30 of the 300 participants, say the first 10%) and call them an *assessment* dataset and we can call the remaining participants who are in the other nine folds (9 $\times$ 30 = 270 participants) an *analysis* set. Notice this is distinct from the initial split separating the data into training and test sets. Here, we are taking the training data and further splitting/partitioning it into a 10% assessment set and a 90% analysis set. We then set *k* = 3 and see how the model performs (check the accuracy) when we use the 270 participants in the analysis set to predict the 30 participants in the assessment set. That is, take the first participant in the assessment set, add them to the scatter plot with the 270 points in the analysis set, find the three closest participants to the new participant, see if the majority succeed or fail treatment and then compare the prediction vs. the truth for the first participant. This example is shown in [Figure 3](fig:knn_example). We repeat this process for the other 29 participants in the assessment set. We can repeat this entire process on the next 10% fold, then the next fold, until all the participants in the training data set have been included/used in an assessment set. When we have finished, we can average the performance across the 10 groups of 30 (what ML aficionados would call the 10 folds) to calculate an overall assessment of performance with *k* = 3. We can then set *k* = 5 and repeat this entire procedure to check the performance across the folds. This process of checking the performance **across the folds/resamples** is called **cross validation**. A machine can iterate over any value of *k* but typically, we check *k* = 3, *k* = 5, etc. out to some fraction of the sample size (a rule of thumb is to use the square root of the sample size; here the square root of 300 is about 17).[^3] This process of checking the model performance across a range of possible options, like the different values of *k* here, is called "tuning" a "hyperparameter." Remember the phrase "tuning hyperparameters" and remember that the tuning is performed using cross validation. Those details are the core that sets ML apart from other modeling methods/mindsets.

[^3]: I am using odd numbers to avoid ties when voting for prediction.

In addition to checking the count of the numbers of neighbors, it is possible to assign different "weights" to the *k* closest participants. That is, it is easy to imagine that the 10 closest participants are valuable for making predictions but within that "circle", the participants who are closest are even more valuable than the more distant participants. So, it is also possible to "tune" many distance weighting functions at the same time the model is tuning the *k*.

Mathematicians have also defined ways to measure distance beyond straight lines. For example, traveling in a city does not involve moving in a straight line "as the crow flies", rather, it involves moving up/down and right/left some number of blocks (i.e., Manhattan distance). So, the distance between two neighbors can also be optimized using many ways to measure distance. The end result is an optimal *k* with its optimal proximity and distance weighting function. As you can imagine, checking all the possible *k* values with many different distance weights leads to a huge number of combinations.

While it is possible to search every combination of *k* and weights by creating a grid, typically a large number of random combinations are tried to cover the space of possibilities. A "space-filling" design can be used to make sure the randomly chosen values are not too clumped together. You will see examples of grids in the code below for methods that are not tuning many hyperparameters (e.g., LASSO) as well as space-filling designs for methods that need to tune many parameters (e.g., neural networks). The state-of-the-art in ML focuses on how to choose options and then check other possible values in the neighborhood that look promising using *Bayesian optimization* methods. Watch for those Bayesian methods in future work.

Once we know the best *k* along with its best weighting function across the training data subsets we can use the test data *once* to see how our model performs on new data. This is how ML works. Different algorithms require different hyperparameters to be tuned, but at its core, ML is all about checking the model performance on samples of the training data (after it is split into numerous analysis and assessment sets).

There are many ways to create analysis and assessment sets. Splitting the training data into tenths and using 10-fold cross validation is a very popular option for huge datasets. Another popular alternative, which we have done in the paper, is 5-fold cross validation. It is popular because it helps decrease computing time and it ensures that the assessment data has enough successes if the dataset is small or if the outcomes are extremely rare. Another very common option is to use bootstrap resamples. With the bootstrap, an analysis set is created by randomly sampling a number of participants *with replacement* to make an assessment set which has the same number of participants as the original data. *With replacement* means that after an individual is selected, they are returned to the pool of eligible participants to be selected. So, one person can be selected two or more times in a bootstrap sample. Because bootstrap can select the same person more than once (after they were replaced) and because the requirement is that the assessment set is the same size as the original data, there will be some participants who were not selected as part of the analysis set. They can be used to assess the performance of a model applied to the analysis set. It can be mathematically shown that a bootstrap resample will generate an analysis set that contains about 2/3 of the original records, leaving 1/3 of the records for the assessment set. To learn more, see *An Introduction to Statistical Learning* [@james2021a].

## Explanations of the Methods Used in the Paper- Without Code

### *k*-Nearest Neighbors - Prediction Without Learning

Along with the other methods we explore here, we can make a statement about which variables are relatively influential. For some methods, like the regression models which we will cover next, we can make strong statements about what happens to the outcome as predictors change. With kNN, we gain none of those benefits. We may get a good prediction, but we don't know why it works beyond saying that "this person responds like similar people." This lack of transparency in the relationship between the predictors and the outcomes has led kNN to be labeled as a "black box" method. Whereas ML experts have been able to explore the inner workings of many ML algorithms that were once considered inscrutable black box methods, the kNN algorithm remains a closed black box. In essence, kNN will yield predictions, but we learn nothing about the role and influence of the individual predictors.

### Regression Modeling

The ML community has embraced and expanded the linear modeling methods that most people are taught in introductory statistics classes. Unlike the other methods we explore below, traditional ordinary least squares regression and logistic regression models do not require tuning of hyperparameters. However, leaders in the field have expanded and modified the traditional approaches to take advantage of ML methods.[@james2021a]

#### Traditional Logistic Regression

While the algorithms used to do logistic regression in the context of ML modeling are the same as traditional statistical methods, their practical implementation gains when applying a ML framework. Rather than fitting a single model using all the available data, as is often done by classically trained practitioners, those using ML will split into training and testing sets, then use cross-validation or bootstrapping and then evaluate the model on the test data. In this context, logistic regression becomes just another competitor which is evaluated to see if it does the best job predicting the data. While traditional logistic regression cannot readily detect the complex patterns in the data which other methods note, it has not fallen out of favor because it offers relatively simple explanations in terms of the change in the odds of an outcome as each predictor changes. Models with simple explanations are called parsimonious.

The traditional "manual" modeling process historically taught in epidemiology classes is unpopular in the ML framework. That process, which exploits subject matter knowledge to select variables to include in a prespecified order (typically by looking at the impact of each predictor by itself, then including variables that are statistically significant in other models), followed by manually removing variables (often based on a *p*-value based criteria), has a tendency to produce model results which do not replicate when applied to new datasets. Instead of relying on *p*-value-based metrics, some practitioners will use automatic "stepwise" modeling procedures that are optimized based on criteria other than *p*-values, like AIC. Even these automatic stepwise procedures are being supplanted by ML "shrinkage" methods like Least Absolute Shrinkage and Selection Operator (LASSO).[@smith2018; @james2021a]

#### Logistic LASSO

LASSO and its cousin, ridge regression, conceptually begin by saying the effects that we note based on this sample are going to be too large. That is, if we were to gather another sample, the same predictors may matter, but extra large effects, which may happen by chance associations in this sample, need to be shrunk toward zero. So, while traditional logistic regression is optimized to find the optimal "betas" to describe the impact of the predictor in this sample,[^4] LASSO is designed to simultaneously find the most accurate betas for this sample and shrink them down toward results with the magnitude we expect to see in a new sample. The LASSO adjustment "penalty" pushes all the betas toward zero and some of them are shrunk to be zero, meaning that as the predictor changes, the outcome is not influenced. Because LASSO can be used to simultaneously produce conservative estimates of the impact of each predictor and remove unimportant variables (drop some betas to zero), many ML practitioners use LASSO instead of the traditional logistic or automatic stepwise modeling methods. That useful ability of eliminating variables because their betas have been shrunk to zero is called variable selection. That said, because so much of the biomedical literature has used traditional logistic methods, we still find it useful to use both traditional and shrinkage regression methods so we can compare our effect sizes with previous work.

[^4]: Betas are the coefficients/numbers that describe the impact of each predictor in a model. For example, a regression model describing the number of needles exchanged in a harm reduction program could say the number of needles exchanged is increased by on average 2.5 if the person is a man. In that case, the beta is 2.5.

Like with kNN and the logistic modeling, the LASSO modeling process involves splitting the training data into analysis and assessment sets, fitting a series of models on the cross-validation (or bootstrap sets) and evaluating at the performance on the assessment sets. The key difference between this and kNN is that with LASSO, the tuning parameter is the optimal amount of shrinkage. That is, the training data is split into many subgroups and the model performance is assessed across a range of shrinkage penalties. Once the optimal amount of shrinkage is found on the training data, a prediction can be generated on the test set and the performance can be evaluated.

#### Logistic Regression with Resampling

While traditional logistic regression is done by fitting a single model with all the training data, it is possible to fit a LASSO model with the penalty set to be extremely small. By doing this, it effectively just does traditional logistic regression but, because the model is fit using the resampling framework needed for LASSO, we can calculate the cross-validation/resampling estimates of future model performance. That is, the average logistic performance can be calculated across the resampling folds and we can get a predicted ROC AUC.

#### MARS

Both traditional and LASSO regression methods assume that there is a linear (or linear in the logit space) impact of continuous predictors. That is, for every one unit increase in a predictor, there is the same impact (increase or decrease of the odds) on the outcome. Many methods have been proposed to relax this constraint. A popular ML option is Multivariate Adaptive Regression Splines (MARS) models. Rather than saying the impact of a predictor must be described by a line, MARS models allow the line to change its slope in a few places. Imagine it as adding a "hinge" to a traditional regression line. This theoretical hinge, which attaches two line segments, acts like a physical hinge attached to a door allowing it to change the angle by which the door is attached to a wall. This is extremely powerful because we can also describe a different impact of a one unit increase in the predictor across different ranges of the predictor. MARS models are designed to find the breakpoints and add a "hinge" to the regression line where the change happens, thus allowing the person evaluating the model to say the odds change by some known amount *below* the threshold and then by a different amount *above* the threshold. For example, the chances that someone will stop breathing may go up for each miligram (mg) equivalent of morphine across a broad range of doses, but after hitting a threshold, the chances will skyrocket for each additional mg. In this example, there may not be a clear physiological breakpoint. In theory, an optimal model would add a curve to the regression model where the change happens, but that flexibility comes with the cost of difficulty in interpretation. MARS models sidestep that complexity by saying the line has one slope until a threshold is met and then has a different slope.

#### Support Vector Machines

At their core, Support Vector Machines (SVMs) seek to find an optimal "dividing line" that splits data into groups. That dividing line can be a line through a two-dimensional scatter plot, a plane that slices though a three-dimensional cube of data or a hyperplane that cuts across four or more dimensions. SVMs strive to maximize space around the dividing line (i.e., the margin between the boundary and the nearest data points on either side of the line). These nearest points, called support vectors, are crucial in defining the dividing line and giving the algorithm its name.

The true power of SVMs lies in their ability to handle nonlinear decision boundaries through the "kernel trick." To understand this, imagine a scatter plot that shows a circular blob in the middle with a lot of minus symbols representing participants who fail treatment, and all around the blob are many plus symbols representing participants who succeed in treatment. Most ML methods will struggle to make correct predictions with this kind of data because there is no straight line (or step function) that can form a boundary. Imagine that the scatter plot described above is printed on a stretchy rubber surface. The SVM will pinch the middle of the blob for failure of treatment and stretch it up off the page. After the data has been pulled into the third dimension, it will find the optimal plane to slice the data into successes and failures. In other words, this mathematical technique allows SVMs to implicitly map data into a higher dimensional space where linear separation becomes possible. Common kernel functions to do the "stretching" include polynomial, radial basis function (RBF), and sigmoid, each offering different ways to transform the feature space. (NOTE: The many types of kernel functions are beyond the scope of this supplement.) This flexibility enables SVMs to capture more complex patterns than the circular blob that I describe.

In the paper, we use a relatively simple SVM that is tuned for the degree of the polynomial use to change the space and the penalty associated with putting a point on the wrong side of the dividing line.

### Tree-Based Modeling

Tree-based methods begin with the assumption that there are homogeneous groups of participants, and these groups can be identified by building a series of rules that are framed as yes/no questions. These techniques find the optimum yes/no splits in the predictor variables to group participants with the same outcomes. Here, we will be using tree-based methods to find homogeneous groups of participants, that is, participants with similar values on predictor variables who share a common outcome, that succeeded or failed in treatment. Be aware that tree-based methods can also be used to find participants with similar numeric scores, like the number of needles exchanged at a harm reduction program, or to identify groups of participants who have a similar pattern of events through time, like the time until an overdose. In other words, tree-based methods can be used to solve classification, regression, and time-until-event/survival problems. Tree-based methods are particularly useful because their construction intrinsically provides insight into which variables are important and they can pick up on complex (non-linear) trends. See Greenwell's *Tree-based methods for statistical learning in R* for an extremely well written introduction to tree-based models.[@greenwell2022]

#### CART

To understand how the yes/no rules are built, consider how CART (Classification and Regression Trees), one of the most influential early tree-based methods, works to predict treatment success with three variables: age in years, race (i.e., White, Black and Other) and sex (i.e., male or female assigned at birth). CART uses every variable to make every possible yes/no split and then checks the percentage of participants in the two groups who are treatment successes. So, it evaluates a rule that says, "Is this person male?" and it checks to see what percentage of participants in the two groups, male and female, are correctly labeled as treatment successes. It then looks at the age variable and notes the youngest participant is 15. It makes a rule that says, "Is this person 15 or younger?" and it checks the percentage of participants who are treatment successes in the two age groups. Next, it notices there is a 17-year-old, so it makes two groups using the rule, "Is the person 17 or younger?" Again, it checks the percentage of participants who are correctly classified as treatment successes in the two age groups. It repeats this for every possible age split. It then turns to race and makes every possible two-way split. That is, it checks the percentage of participants correctly classified as treatment successes if it splits on Black vs. a group containing both White and Other. Then it checks the percentage correctly classified if it splits on White vs. a group with both Black and Other. Finally, it checks Other vs. a group with both White and Black. Each time, CART notes the percentage who are correctly labeled as treatment successes in each subgroup. Here we are working with only three variables, but CART and other tree-based methods will iterate through and make every possible two-way split in each variable. Once the process is completed, it finds which split makes the most accurate prediction. That is, it picks the split that will result in the lowest probability of incorrectly labeling a randomly chosen participant.

[Figure 4](#fig:tree_example) shows a hypothetical CART "tree" diagram. Note that decision trees are typically drawn upside down, with the "root" of the tree at the top and the final groups, which are called "leaves", drawn at the bottom of the graphic. Real classification trees may provide additional details, like the number of participants in the intermediate decision nodes and the proportion at each intermediate node that are treatment successes or failures. In this example, the algorithm finds that the split of 54 years or younger vs. 55 and older results in the fewest incorrect labels of treatment success and failure. That will be the first (top) split in the tree. It then takes *only the participants in the younger group* and checks the accuracy of the labels if it splits on sex, all the combinations of race, and every possible age split in this younger group. If it can improve the quality by adding another split, the CART method will do it. It chooses the split that reduces the outcome label errors the most. It then turns to the older group and checks every possible split. It does this over and over. It is extremely important to note that with tree-based methods like CART, different variables can play an important role in only some subgroups. It may happen that in the older group race may be the next most important split while in the younger group, sex matters the most to form the second split. CART and other tree-based algorithms can reuse the same variable, so you can see an initial split on age, followed by race, then by age again (corresponding to a differential treatment response only in say middle-aged Black people, for example).

{{< pagebreak >}}

```{r}
#| fig-cap: |
#|   Figure 4: A hypothetical tree predicting treatment success for a cohort
#|   entering treatment for opioid use disorder.
#| echo: false

# knitr::include_graphics("tree.svg")
knitr::include_graphics("./ext/tree.svg")
```

{{< pagebreak >}}

The ability to inspect splits allows looking at differential treatment effects. For example, a tree could split a cohort into people who received treatment *medication one* vs. *other medications*. The most important predictors, signified by additional splits, for the *medication one* group could be age, smoking and IV drug use. The *other medications* group could instead split on study site, days in detox and amount of prior opioid use. CART (and all tree-based methods) allows us to see when completely different predictors could be emphasized for each of the treatment medications.

One obvious way to make homogeneous groups is to keep splitting until each final group (called the leaves of a tree) contains a single participant. When should the tree building process end? There are two general strategies, early stopping rules or pruning full trees. Early stopping rules can take the form of controlling the depth of the tree by dictating the number of splits in advance (say, only allowing three nested subgroups) or preventing the tree from forming subgroups if there are not many participants in a group.

The alternate strategy, pruning a full "bushy" tree, takes advantage of resamples of the training data. After building out the full tree on an analysis subset of the training data, the quality of the model is checked using the assessment set after different amounts of the tree, the portions that contribute the least to the improvement of classification, are cut off. This post-processing of a bushy tree avoids missing important signals that can be unnoticed if a tree stops growing after a small number of splits, for example, limiting a tree to only 3 splits. While it costs extra time, it is also possible, as we did in the paper, to tune both options.

In addition to using model agnostic methods, like permutation-based VIP metrics to determine which variables are important for a prediction, there are model specific, here CART-specific, methods for assessing variable importance. The tree building process tells us which variables are the most important for classifying participants. Splits close to the root of the tree are the most important for classification of treatment success or failure, but variables that are used to split over and over are also important. The ability to use "height in the tree", or the number of times a variable is "used", leads people to view tree-based methods as having intrinsic properties that inform us of a variable's importance. Another extremely useful feature of this process is that the trees don't care about "linear trends" traditionally captured by regression beta parameters. That is, there are no statements about what happens to the outcome if a predictor increases by one point. Rather, there are nonparametric statements about being in one group or another. This means that CART can pick up complex, nonlinear patterns but it will "miss the point" if there actually is a linear trend.

While CART remains popular, especially among statisticians, there are other popular algorithms for building classification trees. These include the C5.0 algorithm, which is similar to CART but uses entropy or information gain to measure the quality of group predictions, and the GUIDE algorithm. These algorithms help to avoid a common issue where CART will tend to overvalue a (categorical) predictor that has many levels. CART's popularity is driven in part by the appeal of a single easy-to-understand set of rules. However, CART and other single tree methods generally perform poorly compared to the predictions generated using sets of trees, like random forests, boosted trees, and BART.

#### Random Forest

Where single tree methods like CART use all the predictors and participants to make a prediction, random forests repeatedly sample both the participants, a process called bagging (i.e.,**b**ootstrap **agg**regat**ing**) and randomly sample predictors as they build many trees. By randomly including portions of the data, predictions coming from random forests devalue highly influential participants and correlated predictors, because they will not always be used when making a tree. Whereas single tree methods typically tune the complexity of trees (e.g., tuning the amount of pruning), random forests, like those in the paper, typically tune the number of variables that are available at each split in the tree as well as the number of participants who are needed to make further splits in the tree.

Because a random forest can include hundreds of trees, we lose the benefit of visually inspecting "the tree" to see which variables are important. These ensembles of tree methods take advantage of the model agnostic methods, like the permutation-based variable importance scores mentioned above, to describe which variables matter to the prediction.

#### Boosted Trees (XGBoost)

Where random forests typically gain strength over single trees by aggregating many trees using randomly selected variables an alternative is to build a tree, check where it does a poor job and then build additional trees that focus on improving the areas where the model is performing poorly. Like random forests, boosted trees can take advantage of permutation-based variable importance scores to specify which variables are critical for predictions.

XGBoost is one such modern algorithm that learns from its mistakes as it builds trees. It and its cousins LightGBM and CatBoost are wildly popular, because of both their speed and accuracy in machine learning competitions.[@xgboost2024] Effectively, it builds a tree, looks to see where the modeling is doing a poor job, and it refines the tree structure to handle those cases that are poorly predicted. It is controlled by many of the same hyperparameters as discussed above but it adds a learning rate parameter that controls how much influence each successive tree has on the model as it tries to correct errors.

#### BART

Bayesian Additive Regression Tree (BART) algorithms are at the leading edge of ML. They leverage an entirely different set of statistical theory (Bayesian inference) and incorporate other methods like LASSO's shrinkage estimators for calculating the complexity of trees. Unlike other tree-based methods that require careful tuning for how to prune back overly bushy trees, the BART algorithms "automatically" figure out these details. BART trees tend to perform well compared to other algorithms and they are strong candidates when the goal is to estimate causal relationships. [@hill2020]

#### Neural Networks

Neural Networks (NNs) are a class of algorithms that include popular complex variants like Deep Learning or Deep Neural Networks. They are extremely popular for solving problems where the data do not come as a traditional statistical dataset (i.e., image or audio processing or complex text). They work using interconnected layers of artificial neurons, with each layer building upon the features learned by previous layers, allowing the model to capture increasingly complex patterns. Their ability to automatically extract relevant features from raw data makes neural networks particularly powerful for complex problems. However, they often require large amounts of computational resources, and their complex structure can make them difficult to interpret compared to simpler models. [@burkov2019]

## Application of Methods with Code

### The Role of Training and Testing Datasets

Here we used a 75% training (N = `r nrow(a_train) |> scales::comma()`) and 25% testing (N = `r nrow(a_test)`) set split of the `r nrow(analysis) |> scales::comma()` total participants. This split was stratified to make sure that we had approximately the same proportion of participants who relapsed in both datasets.

:::: {.callout-note title="Train and Test Split" collapse="true" icon="false" appearance="minimal"}

```{r show-split}
#| eval: false

set.seed(1)

a_split <- initial_split(analysis, strata = "did_relapse")

a_train <- training(a_split)
save(a_train, file = "data/a_train.RData")

a_test <- testing(a_split)
save(a_test, file = "data/a_test.RData")
```

::: {.callout-note title="More Info on the Train and Test Split" collapse="true" icon="false" appearance="minimal"}
The `set.seed()` function specifies which set of random numbers will be used for all tasks that involve randomization. The `initial_split()` function assigns each participant to be a member of the training or testing dataset. The `training()` and `testing()` functions create the datasets.
:::
::::

After splitting the data into the train and testing sets, the training data is divided into five analysis and assessment folds to allow the cross validation of the models described above.

```{r cross-validation}
#| eval: false
a_fold <- vfold_cv(a_train, v = 5)
```

### Preprocessing Recipe

Prior to modeling, all data were preprocessed using the same steps which were programmed using the R `recipies` package (version `r packageVersion("recipes")`). That code, which is shown in the [Workflow]() section above, is repeated here.

::: {.callout-note title="preprocess_recipe.R" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/preprocess_recipe.R >}}
```
:::

The recipe we specify is applied to each subset. As discussed above, this logic, rather than doing the preprocessing step once on the complete data (i.e., the full original dataset), is critical to ensure that the algorithm is not learning from other subsets. That potential problem, called leaking information, could cause the model to give optimistic estimates of future performance. As mentioned in the paper, the steps we take here are to remove any predictor that has nearly no variability with `step_nzv(all_predictors())`. Then any character string is converted to a factor with `step_string2factor(all_predictors())`. Logically, this step is not needed but it is in place to deal with a software [weakness/bug](https://github.com/tidymodels/recipes/issues/926){target="_blank"}. Any missing predictor values are imputed using the KNN algorithm `step_impute_knn(all_predictors())`. Categorical variables are dummy-coded `step_dummy(all_nominal_predictors())`. Extremely rare levels of nominal categorical predictors (levels with less than 5% frequency) are pooled into an "other" category `step_other(all_nominal_predictors())`. Next, the set of all numeric variables are examined and those that were correlated above 0.9 are removed using an algorithm that keeps as many variables as possible `step_corr(all_numeric_predictors())`.[@recipes] Finally, numeric variables are normalized `step_normalize(all_numeric_predictors())`. While not every predictive method requires all of these steps, we use a common recipe to make sure that every model was able to use the same predictors for the same participants.

If you are curious, you can study several alternative recipes that we tried by looking in the ***other_recipes.R*** file.

::: {.callout-note title="other_recipes.R" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/other_recipes.R >}}
```
:::

### Algorithms

#### KNN {#knn}

The *k*-nearest neighbor analyses were conducted using the `kknn` package (version `r packageVersion("kknn")`). The model selected possible values of *k* between 1 and 50 along with the 10 default distance metrics provided in `tidymodels`.

:::: {.callout-note title="Show KNN Code" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/knn.R >}}
```

The first sets of code are specifying the kind of model we will use and then tuning the hyperparameters.

::: {.callout-note title="More info on `KNN Tuning`" collapse="true" icon="false" appearance="minimal"}
The `nearest_neighbor()` function specifies the hyperparameters that need to be trained for the model. The arguments for `nearest_neighbors` function include the number of `neighbors` to evaluate, as well as the `weight_func` and `dist_power` details which influence the importance of close proximity of the neighbors.

The `grid_latin_hypercube()` is used because there are so many possible combinations of neighbors, weight functions and distance powers. It specifies that rather than trying every possible combination of the number of neighbors, weighting functions and distance power functions, we will use 50 values that are randomly selected to make sure that the choices are representative of all the combinations.

The `knn_workflow()` function specifies that the recipe and grid will be used together.

We actually tune the hyperparameters using the `tune_grid()` function.
:::

After running the model, we extract and save the results from the modeling objects. As discussed in the paper and shown [below](#get-training-results), even though it is not normally done, we save results from applying the model to the original training data as well as the testing data. In a normal workflow, we would fit the model, collect the metrics from the training data, and choose to use the optimal values. If this algorithm is the best, then we would apply the tuned hyperparameters to the test data. See below for an explanation of the code.
::::

#### Logistic

A traditional logistic model was built using the `glm` function from the r `stats` package (version `r packageVersion("stats")`)

:::: {.callout-note title="Show Logistic Regression Code" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/logistic.R >}}
```

::: {.callout-note title="More Information on Logisic Regression" collapse="true" icon="false" appearance="minimal"}
While the algorithms used to do logistic regression in the context of ML modeling are the same as traditional statistical methods, their practical implementation gains from a ML framework. Rather than fitting a single model using all the available data, as is often done by classically trained practitioners, people doing ML will typically set aside a portion of the data calling it a “test” set, build a model on the remaining “training” data (or average the result of a set of models generated by using cross validation or bootstrap techniques), and then evaluate the model on the test data by computing a metric like classification accuracy or ROC AUC. In this context, logistic regression becomes just another competitor which is evaluated to see if it does the best job predicting the data. While traditional logistic regression cannot readily detect the complex patterns in the data which other methods note, it has not fallen out of favor because it offers relatively simple explanations, in terms of the change in the odds of an outcome as each predictor changes.

That said, the traditional “manual” modeling process historically taught in epidemiology classes is unpopular in the ML framework. That process, which exploits subject matter knowledge to pick which variables to include in a prespecified order (typically by looking at the impact of each predictor by itself, then including variables that are statistically significant in other models), followed by manually removing variables (often based on a p-value based criteria), has a tendency to produce model results which do not replicate when applied to new datasets. Instead of relying on p-value-based metrics, some practitioners will use automatic “stepwise” modeling procedures that are optimized based on criteria other than p-values, like AIC. Even these automatic stepwise procedures are being supplanted by ML “shrinkage” methods like LASSO.
:::
::::

#### LASSO Logistic

LASSO models were built using the glmnet package (version 4.1.8) with 30 different levels of regularization penalty, or "shrinkage", between .001 and 1.

:::: {.callout-note title="Show LASSO Logistic Code" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/lasso.R >}}
```

::: {.callout-note title="More information on LASSO logistic" collapse="true" icon="false" appearance="minimal"}
Tuning LASSO regression models involves one parameter: `penalty` (the amount of regularization). The same function in R can also be used to fit a ridge regression or elastic net model. The code we are using sets `mixture = 1`, to specify our use of a pure LASSO model. The LASSO technique is also commonly referred to as L1 regularization. Ridge regression (L2 regularization) sets `mixture = 0` and elastic net regressions use `mixture` values between 0 and 1. [@james2021a]

The `tune_grid()` function computes ROC AUC for various LASSO shrinkage (`penalty`) values. [@hastie2023] The `penalty` parameter represents the amount of shrinkage. The impact of the shrinkage will be assessed across 30 levels with log-scaled values from -3 to 0. The goal of applying these penalties is to shrink the coefficient values (the beta values) towards zero. The result is that some coefficients may be set to zero, meaning the variable does not impact the outcome, while others will have their magnitude reduced. In other words, the variables with the greatest impact on treatment outcome will remain important, though with a possibly dampened effect. This is an effective application of feature reduction.

The level of regularization chosen for the model is the one that produces the greatest value for the ROC AUC with cross-validated resamples, then the final workflow is fit to the training data. To obtain exponentiated beta estimates corresponding to odds ratios (ORs), the `lasso_final_fit` object holding the model results is sent to `tidy(exponentiate = TRUE)`. The value of the OR is the change in odds of the outcome for a one-unit increase in the predictor variable. We apply more cleaning steps by filtering to only non-zero variables (`filter(estimte != 0)`) and `select`ing the `term` (named variable) and `estimate`, which now is the OR. Finally, the results are `arrange`d in `desc`ending value of OR which sorts the highest ORs to the top of the table and draws the reader's attention to the variables with the greatest impact on the relapse outcome.
:::
::::

#### Logistic with Resampling

We can trick the tidymodels ecosystem to use cross-validated resamples by using the infrastructure for LASSO but force tidymodels to not apply the full LASSO technique. Basically, we are saying use the LASSO code but don't try different levels of shrinkage. Use only a tiny amount.

:::: {.callout-note title="Show Logistic with Resamping Code" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/logistic_via_lasso.R >}}
```

::: {.callout-note title="More information on logistic with resampling" collapse="true" icon="false" appearance="minimal"}
As discussed above tuning LASSO regression models involves one parameter: `penalty` (the amount of regularization) and setting `mixture = 1`. A limiting factor of tidymodels is that it does not discretely allow for fitting a logistic regression model with resampling. In order to use the cross-validated resamples in a logistic regression model, the `penalty` parameter *should not* be tuned. We achieve this by first setting the engine to use LASSO's `glmnet` engine and creating a `grid_regular()` with one level and a one-value range (`range = c(-10, -10)`). In essence, the model will be tuned to only a `mixture = 1` and `penalty = -10`. The net effect is that tidymodels regards our model as a LASSO model, but since we have set it so no penalty is being applied to the variables, it can therefore fit resamples as a logistic regression model.

The chosen model fit is the one that produces the greatest value for the ROC AUC with cross-validated resamples, then the final workflow is fit to the training data. To obtain exponentiated beta estimates corresponding to odds ratios (ORs) we pipe (i.e., `|>`) the results to the `lasso_final_fit` object to `tidy(exponentiate = TRUE)`. The value of the OR is the change in odds of the outcome for a one-unit increase in the predictor variable. We apply more cleaning steps by filtering to only non-zero variables (`filter(estimte != 0)`) and `select`ing the `term` (named variable) and `estimate`, which now is the OR. Finally, the results are `arrange`d in `desc`ending value of OR which sorts the highest ORs to the top of the table and draws the reader's attention to the variables with the greatest impact on the relapse outcome.
:::
::::

#### SVM

::: {.callout-note title="Show Code for `SVM`" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/svm.R >}}
```
:::

#### CART

CART models were built using the `rpart` package (version 4.1.23) with trees tuned on tree depth (1, 15), minimum number needed to split (2, 40) and cost complexity (log10 scale between -10 and -1).

:::: {.callout-note title="Show Code for `cart_spec`" collapse="true" icon="false" appearance="minimal"}
```{r show-cart-spec}
#| eval: false
cart_spec <-
  decision_tree(
    tree_depth = tune(), min_n = tune(), cost_complexity = tune()
  ) |>
  set_engine("rpart") |>
  set_mode("classification")
```

::: {.callout-note title="More Info" collapse="true" icon="false" appearance="minimal"}
The `decision_tree()` function fits both classification and regression models and creates a tree-based structure. Three parameters(`tree_depth`, `min_n`, and `cost_complexity`) were tuned in order to obtain a decision tree model that better captures the underlying patterns of the data. “rpart” is the package `set_engine()` used for building the classification and regression trees by implementing recursive partitioning.
:::
::::

:::: {.callout-note title="Show Code for `cart_workflow`" collapse="true" icon="false" appearance="minimal"}
```{r show-cart-workflow}
#| eval: false
cart_workflow <- workflow() |>
  add_recipe(a_recipe) |>
  add_model(cart_spec)
```

::: {.callout-note title="More Info on `a_recipe`" collapse="true" icon="false" appearance="minimal"}
The steps added to the `a_recipe` object include: a near-zero variance filter, KNN imputation on all predictors, dummy coding on all nominal predictors, a step that creates an “other“ category to house infrequently occurring values, a high correlation filter, and a step to normalize numeric data to have a standard deviation of one and a mean of zero.
:::
::::

#### Random Forest

::: {.callout-note title="Show code for random forest" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/rf.R >}}
```
:::

#### BART

::: {.callout-note title="Show Code for `BART`" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/bart.R >}}
```
:::

#### Extreme Gradient Boost (XGBoost)

::: {.callout-note title="Show Code for `XGBoost`" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/xgboost.R >}}
```
:::

#### Neural Networks

:::: {.callout-note title="Show Code for `Neural Networks`" collapse="true" icon="false" appearance="minimal"}

```{.r}
{{< include ext/modeling/nnet.R >}}
```

::: {.callout-note title="More Info on neural networks" collapse="true" icon="false" appearance="minimal"}
Tuning neural network models in tidymodels requires the selection of several key parameters: `hidden_units` (the number of nodes in the hidden layer), `penalty` (the amount of regularization), and `learn_rate` (the step size for the optimization algorithm). The `penalty` parameter controls the model's complexity by adding a cost to large weights, helping to prevent overfitting. A higher penalty leads to simpler models, while a lower penalty allows for more complex patterns to be captured. The `learn_rate` determines how much the model adjusts its weights in response to the estimated error each time the model weights are updated. A higher learning rate means faster learning but risks overshooting the optimal solution, while a lower rate learns more slowly but can lead to more precise convergence.[@tidymodels2020]

The nodes that reside in the hidden layer of the neural network lie between the input and output layers and are tasked with performing computations to detect complexities in the data. We use the `mlp()` function to specify a multilayer perceptron model, a type of feedforward artificial neural network. In this method, the computations performed in the hidden units are passed forward to the next layer.

The `epochs` parameter is set to 100, which determines the number of times the learning algorithm will work through the entire training dataset. The `hidden_units`, `penalty`, and `learn_rate` parameters are set to `tune()`, indicating that we want to find optimal values for these parameters. We use the "brulee" engine to model a neural network and set the mode to "classification" for our binary outcome prediction.

The `grid_latin_hypercube()` function is used to create a tuning grid with 30 different hyperparameter combinations, providing a good balance between exploration of the parameter space and computational efficiency. This method ensures a good coverage of the parameter space: `hidden_units(range = c(10, 100))`, `penalty` ranges on a log scale from -5 to 0, and `learn_rate` ranges on a log scale from -5 to -1. We use log scales for `penalty` and `learn_rate` to explore a wide range of values efficiently.

The workflow combines our recipe (`a_recipe`) with the neural network model specification. The `tune_grid()` function then trains models for each combination of hyperparameters in the grid, using cross-validation (`a_fold`). We evaluate the models using multiple metrics: ROC AUC, accuracy (the proportion of correct predictions), and Cohen's Kappa (which measures agreement between predicted and actual classifications, adjusting for agreement by chance). The result of this tuning process will be a set of models with different hyperparameter combinations, evaluated across these metrics. The best performing model can then be selected based on the ROC AUC value. A confusion matrix and ROC plot is then generated to visualize the performance of the neural network model.

There are, however, caveats for using a modeling technique as a neural network. Foremost, their interpretability is often difficult as they are regarded as a "black box" model, making it relatively difficult to understand how they arrived at their predictions in comparison to logistic regression or other linear modeling methods. Second, neural networks are computationally expensive, meaning that training times are longer, especially with larger datasets, than random forest or boosted gradient models. Third, neural networks require tuning many parameters to achieve optimal performance. Comparatively, our LASSO regression was tuned only for optimal `penalty` whereas our neural network demanded a deeper understanding of three complex parameters.

We chose to include this complex model because we wished to evaluate its performance against the more conventional methods used in our area of research, but we were unable to achieve greater performance metrics with a neural network. It is important to and reiterate the importance of careful model tuning and interpretation when equipped with other comparative models.
:::
::::

## Results

One of the major benefits of using the `tidymodels` ecosystem of R packages is that the same code can be used to to evaluate the performance of every algorithm. Normally the cross-validation results are used to pick a "winning" algorithm that has the best performance (on a metric like accuracy or ROC AUC) and then the testing data is used once to give an estimate of future model performance on new data. Here, because we wanted to use the data for teaching, we applied the tuned algorithms back to all the training data. As is shown in the paper, the trained models pick up on the true signal but also idiosyncratic details. This shows in the paper that when the ROC AUC produced on the full training data it is superior to the cross validation results. We also show what would have happened if we applied all the models to the test data. Outside of a teaching environment this is unwise because it could lead people to select a model based on the testing results and is likely to produce optimistic estimates of future performance. Below we show the code that was used to extract the modeling details and results from the various algorithms we tried.

Be aware that the `tidymodels` ecosystem has additional functionality called `workflowsets` that further streamline the coding experience by combining many possible recipes and many modeling methods into a cohesive structure. That framework also allows for rigorous statistical comparisons to highlight which methods outperform others. If there is interest, a future project will show that code base.

### Training {#get-training-results}

After tuning the model using the training data, we extract and save the results from the modeling objects. Even though it is not normally done, we've chosen to save the results from applying the model to the original training data *as well as* the testing data. In a normal workflow, we would fit the model and collect the metrics from the training data. Ultimately, we would apply that tuned model, with the best hyperparameters, to the the test data one time. See below for an explanation of the code.

::::: {.callout-note title="Modeling Results" collapse="true" icon="false" appearance="minimal"}
Above we showed the complete code for [KNN](#knn) and it is repeated here:

```{.r}
{{< include ext/modeling/knn.R >}}
```

::: {.callout-note title="Gathering & Inspecting the Fit Model Results" collapse="true" icon="false" appearance="minimal"}
The `collect_metrics()` function from the `tune` package returns a tibble of the `knn_tune_res`, which contains the KNN tuning results. This allows us to inspect a summary of the model's training results (like the accuracy and ROC AUC) using the resampled data.

The `show_best(knn_tune_res, metric = "roc_auc")` and `select_best(knn_tune_res, metric = "roc_auc")` functions are used to show and save the optimized KNN model, specifically, the top performing model and its performance estimates.
:::

Next we can specify that the future work will use the tuned hyperparameters from our best performing model.

::: {.callout-note title="Choosing the best model" collapse="true" icon="false" appearance="minimal"}
Saving the `finalize_workflow(knn_workflow, knn_details)` object is how we accept and choose the best performing model. Our `knn_final` object is the model that we will use to obtain performance metrics on the training and testing data.
:::
:::::

### Full training data

Finally, we will apply the model to our training data and evaluate its performance. Remember, the model was tuned using the **resampled** data, so this is the first instance of using the **training** data on the model.

:::: {.callout-note title="Use the Model on the Training Data" collapse="true" icon="false" appearance="minimal"}
```{r show-applied-knn-final-fit}
#| eval: false
knn_final_fit <-
  fit(knn_final, data = a_train)

knn_autoplot <-
  autoplot(knn_tune_res) +
  ggthemes::theme_few() +
  theme(text = element_text(size = 30))
save(knn_autoplot, file = "data/knn_autoplot.RData")

# Obtain model metrics
knn_metrics <-
  augment(knn_final_fit, new_data = a_train) |>
  mutate(estimate = .pred_0) |>
  roc_auc(truth = did_relapse, estimate)
save(knn_metrics, file = "data/knn_metrics.Rdata")

knn_conf_mat <-
  augment(knn_final_fit, new_data = a_train) |>
  conf_mat(truth = did_relapse, estimate = .pred_class)
save(knn_conf_mat, file = "data/knn_conf_mat.Rdata")
```

The `fit()` function requires that we pass our training data (`a_train`) to the best performing model (`knn_final`). The `knn_final_fit` object holds the parameter estimates for our model on the **training** data.

::: {.callout-note title="Role of `augment` and `autoplot`" collapse="true" icon="false" appearance="minimal"}
By using the `augment()` function, we return our training data with extra columns with a predicted probability for failure (`.pred_0`) and success (`.pred_1`), and a outcome 0/1 classification (`.pred_class`). This in turn is used to obtain the ROC metric<a id="get-roc-metric"></a> and confusion matrices for the datasets.

```{r show-knn-augment}
#| eval: false
# Obtain model ROC AUC on training data
knn_metrics <-
  augment(knn_final_fit, new_data = a_train) |>
  mutate(estimate = .pred_0) |>
  roc_auc(truth = did_relapse, estimate)
```

Notice that we further clean our `augment`ed training data and create a column `estimate` that is the predicted probability of failure of treatment. If we were to instead use `.pred_1` our ROC AUC will not be correct in demonstrating the model's ability to predict failure of treatment. *Knowing which level of outcome is important in your own prediction modeling.*

`autoplot()` is used to render a plot to visualize how the accuracy, Brier class, and ROC AUC varies across the various combinations of hyperparameters on the resampled data by using the `knn_tune_res` object.

```{r show-knn-autoplot}
#| eval: false
knn_autoplot <-
  autoplot(knn_tune_res) +
  ggthemes::theme_few() +
  theme(text = element_text(size = 30))
save(knn_autoplot, file = "data/knn_autoplot.RData")
```

```{r load-knn-autoplot}
#| echo: false
knn_autoplot + theme(text=element_text(size=10))
```
:::
::::

### Confusion matrix {#conf-matrix}

Next we use the model and display how it performs using a confusion matrix.

::: {.callout-note title="Show `conf_mat` code" collapse="true" icon="false" appearance="minimal"}

```{r show-knn-conf-matrix}
#| eval: false
# Create a confusion matrix of model predictive performance
knn_conf_mat <-
  augment(knn_final_fit, new_data = a_train) |>
  conf_mat(truth = did_relapse, estimate = .pred_class)
save(knn_conf_mat, file = "data/knn_conf_mat.Rdata")
```

:::

Again, we use the `augment` function to obtain the new prediction probabilities and predicted class of the training data. Similar to the `roc_auc` function, the `conf_mat` function requires the declaration of the observed outcome (`truth = did_relapse`) though here we state that the estimate is the `.pred_class` column. This creates a 2x2 cross-tabulated matrix for the observed and predicted classes.

```{r load-knn-conf-matrix}
#| echo: false
#| class: "text-center"
knn_conf_mat
```

The upper left cell is known as true negative where the model correctly predicted the outcome that the participant did *not* relapse. The diagonal lower right is the true positive cell representing the number of times the model correctly predicted those participants that *did* relapse. The off-diagonal cells are the incorrect predictions.

The KNN model correctly predicted the outcome for the `r knn_conf_mat$table[1]` study participants that did **not** relapse, but incorrectly predicted the outcome for `r knn_conf_mat$table[2] + knn_conf_mat$table[3]` total participants.

### ROC {#roc}

::: {.callout-note title="Show `roc_auc` code" collapse="true" icon="false" appearance="minimal"}

```{r show-knn-roc-metrics}
#| eval: false
# Obtain model ROC AUC on training data
knn_metrics <-
  augment(knn_final_fit, new_data = a_train) |>
  mutate(estimate = .pred_0) |>
  roc_auc(truth = did_relapse, estimate)
save(knn_metrics, file = "data/knn_metrics.Rdata")
```

:::

The `roc_auc()` function receives the training data with new predictions and we specify that the observed outcome (`truth`) is in the `did_relapse` column, but assess that against the prediction of `estimate` (the predicted probability of failure).

```{r load-knn-roc-metrics}
#| echo: false
knn_metrics |> gt::gt()
```

Our ROC AUC statistic is very optimistic (`r round(knn_metrics$.estimate, 3)`) when applied to the training data. We will need to compare that against our cross-validated results and, later, against the testing data.

### Test

As we have demonstrated above in the [Confusion matrix](#conf-matrix) and [ROC](#roc) sections, obtaining the results of the testing data is achieved in the same way as for obtaining those results on the training data, except now we supply `new_data = a_test` to the `augment` function. This allows us to apply the same final fitted model (in this case, `knn_final_fit`) to the **testing** data.

::: {.callout-note title="Show Test data `roc_auc` & `conf_mat` code" collapse="true" icon="false" appearance="minimal"}

```{r show-test-roc-conf-matrix}
#| eval: false
knn_metrics_test <-
  augment(knn_final_fit, new_data = a_test) |>
  mutate(estimate = .pred_0) |>
  roc_auc(truth = did_relapse, estimate)
save(knn_metrics_test, file = "data/knn_metrics_test.Rdata")

knn_conf_mat_test <-
  augment(knn_final_fit, new_data = a_test) |>
  conf_mat(truth = did_relapse, estimate = .pred_class)
save(knn_conf_mat, file = "data/knn_conf_mat_test.Rdata")
```

:::

::: callout-important
You will notice that we used the same code to obtain test results as we had for training results, but only changed the `new_data` argument to use `a_test`, our **test** data. This is the beauty of tidymodels: reuse your code structures, but change only a few pieces.
:::

## References

::: {#refs}
:::
